\MyLogo
  \frametitle{Example 1: SoftMax}

\scriptsize{
\begin{lstlisting}[language=python]
# Redefine loss function using softmax as one operator
def train_loss(w, x):
    y = np.dot(x, w)
    prob = softmax(x=y, softmax_label=softmax_label)
    loss = -np.sum(label * np.log(prob)) / num_samples
    return loss

# Initialize weight matrix (again)
weight = random.randn(num_features, num_classes)

# Calculate gradient function automatically
grad_function = grad_and_loss(train_loss)

# Now training it for 100 iterations
start_time = time.time()
for i in range(100):
    dw, loss = grad_function(weight, data)
    if i % 10 == 0:
        print 'Iter {}, training loss {}'.format(i, loss)
    weight -= 0.1 * dw
print 'Training time: {}s'.format(time.time() - start_time)
\end{lstlisting}
}
