%!TEX root = ../talk.tex

\section{TensorFlow}\label{sec:TF}

%%%

\frameinlbffalse

{
\usebackgroundtemplate{
\tikz[overlay,remember picture] \node[opacity=0.8, xshift=-3.5cm, at=(current page.east)] {
\includegraphics[width=0.35\paperwidth]{figures/TF_logo.png}
};}

\begin{frame}[plain]
\frametitle{\S\ref{sec:TF}. \insertsection}
\listofframes
\end{frame}
\addtocounter{framenumber}{-1} % this page does not count

}

\frameinlbftrue

%%%
\subsection{Programming interface}
%%%

\begin{frame}
  \MyLogo
  \frametitle{Basic Concepts}

\begin{itemize}

\item[\raisebox{-0.4ex}{\alert{\HandRight}}] Graph: In TensorFlow, ML algorithms are represented as computational graph. A computational graph (data flow graph) is a form of directed graph where vertices (nodes) describe operations, while edges represent data flowing between these operations.
	  	
\item Operation: An operation may represent a variable or constant, a control flow directive, a mathematical function, a file I/O, or a network communication port.
	  	
\item Tensor: A tensor is a multi-dimensional collection of homogeneous values with a fixed static type.
	  	
\item Variable: Variables can be described as persistent, mutable handles to in-memory buffers storing tensors.
	  	
\item[\raisebox{-0.4ex}{\alert{\HandRight}}] Session: In TensorFlow the execution of operations and evaluation of tensors may only be preformed in a special environment called session.
	  	
\end{itemize}

\end{frame}

%

\begin{frame}
	\MyLogo
	\frametitle{TensorFlow Operations}  

\centering\includegraphics[width=0.95\paperwidth]{figures/TFkernels.png} 
	
\end{frame}

%%%
\subsection{Computational graph}
%%%

\begin{frame}[fragile]
  \MyLogo
  \frametitle{Computational Graph}  
%
\vskip -5pt
\begin{columns}
\column{.75\textwidth}
\begin{lstlisting}[language=python]
import tensorflow as tf

# Define a computational graph for TensorFlow
graph = tf.Graph()

# Define operation nodes in the graph
with graph.as_default():

    with tf.name_scope('input'):
        # Define two constant symbols (two nodes)
        a = tf.constant(1, name='a')
        b = tf.constant(2, name='b')

    with tf.name_scope('output'):
        # Define an operation node: c = a * b
        c = tf.multiply(a, b)
        tf.summary.histogram('c', c)
    
    # Merge all summaries and write to logdir
    logdir = '/tmp/tensorboard'
    merged = tf.summary.merge_all()
    writer = tf.summary.FileWriter(logdir, graph)

with tf.Session(graph=graph) as sess:
    # Compute and output the result (c=a*b)
    result = sess.run(c)
    print result
    # Write TensorBoard data to logdir
    writer.add_summary(merged.eval())

writer.close()
\end{lstlisting}

\pause

\column{.2\textwidth}
%
\begin{figure}[htbp] 
   \includegraphics[height=1.8in]{figures/compgraph.png} 
\end{figure}
%
\end{columns}

\end{frame}

%%%
\subsection{Simple examples}
%%%

\begin{frame}[fragile]
  \MyLogo
  \frametitle{Example: SoftMax in TF}  
 
\begin{lstlisting}[language=python]
import tensorflow as tf

# Import training and test data from MNIST
import tensorflow.examples.tutorials.mnist.input_data as input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

graph = tf.Graph()

with graph.as_default():

    # Nodes can be grouped into visual blocks for TensorBoard
    with tf.name_scope('input_features'):
        x = tf.placeholder(tf.float32, shape=[None, 784], name='input_x')

    with tf.name_scope('input_labels'):
        y_ = tf.placeholder(tf.float32, shape=[None, 10], name='labels')

    with tf.name_scope('parameters'):
        W = tf.Variable(tf.zeros([784, 10]), name='weights')
        b = tf.Variable(tf.zeros([10]), name='biases')
        tf.summary.histogram('WEIGHTS', W)
        tf.summary.histogram('BIASES', b)
    
    with tf.name_scope('use_softmax'):
        y = tf.nn.softmax(tf.matmul(x, W) + b)

    with tf.name_scope('train'):
        # Compute the cross entropy of real label y_ and prediction label y
        cross_entropy = -tf.reduce_sum(y_*tf.log(y))
        # Create a gradient-descent optimizer with learning rate = 0.01
        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
\end{lstlisting}

\end{frame}

\begin{frame}[fragile]
  \MyLogo
  \frametitle{Example: SoftMax in TF (Cont)}  

\ContinueLineNumber
\begin{lstlisting}[language=python]
    with tf.name_scope('test'):
        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
        # Track accuracy over time for TensorBoard
        tf.summary.scalar('Accuracy', accuracy)

    logpath = '/tmp/tensorboard'  # temporary path for storing TB summaries
    merged = tf.summary.merge_all()  # Merge all the summaries
    writer = tf.summary.FileWriter(logpath, graph)  # Write summaries

with tf.Session(graph=graph) as sess:
    # Initialize all variables
    tf.global_variables_initializer().run()
    
    for step in range(1,501):
        if (step%10) == 0:
            feed = {x: mnist.test.images, y_: mnist.test.labels}
            _, acc = sess.run([merged, accuracy], feed_dict=feed)
            print('Accuracy at %s step: %s' % (step, acc))
        else:
            batch_x, batch_y = mnist.train.next_batch(100)
            sess.run(train_step, feed_dict={x: batch_x, y_: batch_y})
            writer.add_summary(merged.eval(feed_dict={x: batch_x, y_: batch_y}), 
                               global_step=step)

    writer.close()

print("Run the command line to start TensorBoard:\n" \
      "(TensorFlow) $ tensorboard --logdir=/tmp/tensorboard" \
      "\nThen open http://0.0.0.0:6006/ into your web browser")
\end{lstlisting}

\end{frame}

%%%
\subsection{Visualization}
%%%

\begin{frame}[fragile]
  \MyLogo
  \frametitle{Visualization: TensorBoard}  

\vskip -25pt
\begin{columns}
\column{.45\textwidth}  
\scriptsize{
\structure{Declarative style}
\begin{itemize}
\item  Same style as Theano
\item  Easy to understand
\item  Possible for graph optimization
\item  \alert{More difficult for debugging}
\end{itemize}

\medskip

\structure{Graphs are powerful but complicated}
\begin{itemize}
\item  Thousands of nodes or more 
\item  Network is deep
\item  Visualization is helpful for debugging 
\end{itemize}
}
%
\column{.5\textwidth}
\begin{figure}[htbp] 
   \includegraphics[height=2in]{figures/graphvisualization.png} 
\end{figure}
\end{columns}

\pause 

To run TensorBoard, use the following command: 
\begin{lstlisting}[language=c,numbers=none] 
$ tensorboard --logdir=path/to/log-directory
\end{lstlisting}
Then navigate your web browser to \alert{localhost:6006} to view the TensorBoard.

\end{frame}

%

\begin{frame}
	\MyLogo
	\frametitle{TensorBoard: Scalars}  

\centering\includegraphics[width=0.95\paperwidth]{figures/scalar.pdf} 
	
\end{frame}

%

\begin{frame}
	\MyLogo
	\frametitle{TensorBoard: Graphs}  
	
\centering\includegraphics[width=0.95\paperwidth]{figures/main_graph.pdf} 
	
\end{frame}

%

\begin{frame}
	\MyLogo
	\frametitle{TensorBoard: Distributions}  
	
\centering\includegraphics[width=0.95\paperwidth]{figures/distributions.pdf} 
	
\end{frame}

%

\begin{frame}
	\MyLogo
	\frametitle{TensorBoard: Histograms}  
	
\centering\includegraphics[width=0.95\paperwidth]{figures/histograms.pdf} 
	
\end{frame}