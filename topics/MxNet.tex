%!TEX root = ../talk.tex

\section{MXNet}\label{sec:MxNet}

%%%

\frameinlbffalse

{
\usebackgroundtemplate{
\tikz[overlay,remember picture] \node[opacity=0.8, xshift=-3.5cm, at=(current page.east)] {
\includegraphics[width=0.35\paperwidth]{figures/mxnet_logo.jpg}
};}

\begin{frame}[plain]
\frametitle{\S\ref{sec:MxNet}. \insertsection}
\listofframes
\end{frame}
\addtocounter{framenumber}{-1} % this page does not count

}

\frameinlbftrue

%%%
\subsection{Programming interface}
%%%

\begin{frame}
  \MyLogo
  \frametitle{Programming Interface}  

\begin{enumerate}
%
\item Support many scope applications (e.g. computer vision, natural language processing,  speech recognition, unsupervised machine learning, support embedded APIs, visualization)
%
\item Mixed programming style: imperative and declarative
\begin{itemize}
\item Data parallelism with multi-devices: Better scalability than TensorFlow, reportedly
\item Support many different front-end, including JavaScript (so it be run on web browsers as well)
\item Provide intermediate-level and high-level interface modules
\item Provide abundant IO functions 
%
\end{itemize}
%
\item Fully compatible with Torch: modules and operators
%
\item Support building neural network graphs
\begin{itemize}
\item Call mx.viz.plot\_network( )
\end{itemize}
%
\item Not well documented
%
\end{enumerate}

\end{frame}

%%%
\subsection{Simple examples}
%%%

\begin{frame}[fragile]
  \MyLogo
  \frametitle{Example: SoftMax in MXNet}  

\begin{lstlisting}[language=python]
import mxnet
import mxnet.symbol as sym
import numpy as np
import numpy.random as random
import time
from minpy.core import function
from minpy.core import grad_and_loss

# define softmax symbol
x_shape = (num_samples, num_classes)
label_shape = (num_samplesm,)
softmax_symbol = sym.SoftmaxOutput(data=sym.Variable('x'), 
                                   name='softmax', grad_scale=1.0/num_samples)

# convert MXNet symbol into a callable function 
# corresponding gradient function
softmax = function(softmax_symbol, [('x', x_shape), ('softmax_label', label_shape)])

# make softmax_label; 
# MXNet's softmax operator does not use one-of-many label format
softmax_label = np.argmax(label, axis=1)

# Redefine loss function using softmax as one operator
def train_loss(w, x):
    y = np.dot(x, w)
    prob = softmax(x=y, softmax_label=softmax_label)
    loss = -np.sum(label * np.log(prob)) / num_samples
    return loss
\end{lstlisting}

\end{frame}

\begin{frame}[fragile]
  \MyLogo
  \frametitle{Example: SoftMax in MXNet (Cont)}  

\ContinueLineNumber
\scriptsize{
\begin{lstlisting}[language=python]
# Initialize weight matrix (again)
weight = random.randn(num_features, num_classes)

# Calculate gradient function automatically
grad_function = grad_and_loss(train_loss)

# Now training it for 100 iterations
start_time = time.time()
for i in range(100):
    dw, loss = grad_function(weight, data)
    if i % 10 == 0:
        print 'Iter {}, training loss {}'.format(i, loss)
    weight -= 0.1 * dw
    
print 'Training time: {}s'.format(time.time() - start_time)
\end{lstlisting}
}

\vskip 100pt

\end{frame}