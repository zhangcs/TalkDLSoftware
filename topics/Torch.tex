%!TEX root = ../talk.tex

\section{Torch}\label{sec:Torch}

%%%

\frameinlbffalse

\begin{frame}[plain]
\frametitle{\S\ref{sec:Torch}. \insertsection}
\listofframes
\end{frame}
\addtocounter{framenumber}{-1} % this page does not count

\frameinlbftrue

%%%
\subsection{Programming interface}
%%%

\begin{frame}
  \MyLogo
  \frametitle{Programming interface}  
\begin{enumerate}
\item Wide range of applications
\begin{itemize}
\item Speech, image and video applications
\item  Large-scale machine-learning applications
\end{itemize}
\item Fastest scripting language Lua is used
\item Easily ported to any platform
\begin{itemize}
\item Torch can run on iPhone with no modification to scripts
\end{itemize}
\item Easy extensibility
\begin{itemize}
\item Easy to integrate any library into Torch
\end{itemize}
\end{enumerate}
\end{frame}

%%%
\subsection{Simple examples}
%%%

\begin{frame}[fragile]
\MyLogo
\frametitle{Example: Two-Layer Network}  
\scriptsize{
\begin{lstlisting}[language=python]
import torch
from torch.autograd import Variable

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold inputs and outputs, and wrap them in Variables.
x = Variable(torch.randn(N, D_in))
y = Variable(torch.randn(N, D_out), requires_grad=False)

# Use the nn package to define our model as a sequence of layers.
model = torch.nn.Sequential( torch.nn.Linear(D_in, H),
                             torch.nn.ReLU(),
                             torch.nn.Linear(H, D_out), )

# The nn package also contains definitions of popular loss functions;
loss_fn = torch.nn.MSELoss(size_average=False)

learning_rate = 1e-4

for t in range(500):
	# Forward pass: compute predicted y by passing x to the model.
	y_pred = model(x)
	# Compute and print loss.
	loss = loss_fn(y_pred, y)
	print(t, loss.data[0])
	# Zero the gradients before running the backward pass
	model.zero_grad()
\end{lstlisting}
}
\end{frame}

\begin{frame}[fragile]
\MyLogo
\frametitle{Example: Two-Layer Network (Cont)}  

\ContinueLineNumber
\begin{lstlisting}[language=python]         
	# Backward pass: compute gradient of the loss
	loss.backward()
         
	# Update the weights using gradient descent
	for param in model.parameters():
		param.data -= learning_rate * param.grad.data
	end
end
\end{lstlisting}

\vskip 125pt

\begin{center}
{\color{red}\scriptsize
https://github.com/jcjohnson/pytorch-examples
}
\end{center}

\end{frame}

%%%

\begin{frame}[fragile]
\MyLogo
\frametitle{Example: Linear Regression in Lua}  
\scriptsize{
\begin{lstlisting}[language=python]
require 'torch'
require 'optim'
require 'nn'

# write the loss to a text file and read from there to plot it as training proceeds
logger = optim.Logger('loss_log.txt')

# input data 
data = torch.Tensor{{40,  6,  4},{44, 10,  4},{46, 12,  5},
{48, 14,  7},{52, 16,  9},{58, 18, 12},{60, 22, 14},
{68, 24, 20},{74, 26, 21},{80, 32, 24}}

# define the container
model = nn.Sequential()                 
ninputs = 2; noutputs = 1

# define the only module
model:add(nn.Linear(ninputs, noutputs)) 

# Define a loss function
criterion = nn.MSECriterion()

# retrieve its trainable parameters
x, dl_dx = model:getParameters()

# compute loss function and its gradient 
feval = function(x_new)
   # set x to x_new, if differnt
   if x ~= x_new then
      x:copy(x_new)
   end
\end{lstlisting}
}
\end{frame}

\begin{frame}[fragile]
\MyLogo
\frametitle{Example: Linear Regression in Lua (Cont)}  
\ContinueLineNumber
\scriptsize{
\begin{lstlisting}[language=python]
   # select a new training sample
   _nidx_ = (_nidx_ or 0) + 1
   if _nidx_ > (#data)[1] then _nidx_ = 1 end

   local sample = data[_nidx_]
   local target = sample[{ {1} }]    
   local inputs = sample[{ {2,3} }] 

   # reset gradients
   dl_dx:zero()
 
   # evaluate the loss function and its derivative wrt x
   local loss_x = criterion:forward(model:forward(inputs), target)
   model:backward(inputs, criterion:backward(model.output, target))

   # return loss(x) and dloss/dx
   return loss_x, dl_dx
end

# define SGD 
sgd_params = {
   learningRate = 1e-3,
   learningRateDecay = 1e-4,
   weightDecay = 0,
   momentum = 0
}

# we cycle 10,000 times over our training data
for i = 1,1e4 do
   #this variable is used to estimate the average loss
   current_loss = 0
\end{lstlisting}
}
\end{frame}

\begin{frame}[fragile]
\MyLogo
\frametitle{Example: Linear Regression in Lua (Cont)}  
\ContinueLineNumber
\scriptsize{
\begin{lstlisting}[language=python]
   #an epoch is a full loop over our training data
   for i = 1,(#data)[1] do
      # return new x and value of the loss functions
      _,fs = optim.sgd(feval,x,sgd_params)
      # update loss       
      current_loss = current_loss + fs[1]
   end      
   
   # report average error on epoch
   current_loss = current_loss / (#data)[1]
   print('current loss = ' .. current_loss)
   
   logger:add{['training error'] = current_loss}
   logger:style{['training error'] = '-'}
   logger:plot()  
end

# Test the trained model
text = {40.32, 42.92, 45.33, 48.85, 52.37, 57, 61.82, 69.78, 72.19, 79.42}

for i = 1,(#data)[1] do
   local myPrediction = model:forward(data[i][{{2,3}}])
   print(string.format("%2d %6.2f %6.2f", i, myPrediction[1], text[i]))
end
\end{lstlisting}
}

\vskip 50pt
\end{frame}
